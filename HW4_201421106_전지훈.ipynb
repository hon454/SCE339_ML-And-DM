{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The current Numpy installation ('c:\\\\users\\\\hon45\\\\documents\\\\github\\\\machinelearning\\\\venv\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-2aea66f4d5ba>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpylab\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdatasets\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mfetch_openml\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0msklearn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_selection\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtrain_test_split\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hon45\\documents\\github\\machinelearning\\venv\\lib\\site-packages\\numpy\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    303\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    304\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplatform\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"win32\"\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmaxsize\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m**\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 305\u001B[1;33m         \u001B[0m_win_os_check\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    306\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    307\u001B[0m     \u001B[1;32mdel\u001B[0m \u001B[0m_win_os_check\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\hon45\\documents\\github\\machinelearning\\venv\\lib\\site-packages\\numpy\\__init__.py\u001B[0m in \u001B[0;36m_win_os_check\u001B[1;34m()\u001B[0m\n\u001B[0;32m    300\u001B[0m                    \u001B[1;34m\"See this issue for more information: \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    301\u001B[0m                    \"https://tinyurl.com/y3dm3h86\")\n\u001B[1;32m--> 302\u001B[1;33m             \u001B[1;32mraise\u001B[0m \u001B[0mRuntimeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m__file__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    303\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    304\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mplatform\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"win32\"\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0msys\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmaxsize\u001B[0m \u001B[1;33m>\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m**\u001B[0m\u001B[1;36m32\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The current Numpy installation ('c:\\\\users\\\\hon45\\\\documents\\\\github\\\\machinelearning\\\\venv\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py') fails to pass a sanity check due to a bug in the windows runtime. See this issue for more information: https://tinyurl.com/y3dm3h86"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 분류 데이터 전처리\n",
    "# mnist_sci = fetch_openml('mnist_784')\n",
    "#\n",
    "# x_train, x_test, y_train_, y_test_ = train_test_split(mnist_sci.data, mnist_sci.target,\n",
    "#                                                     test_size = 0.1,\n",
    "#                                                    shuffle = True)\n",
    "# x_train /= 255.0\n",
    "# x_test /= 255.0\n",
    "# y_train_ = np.array(y_train_, dtype=np.int8)\n",
    "# y_test_ = np.array(y_test_, dtype=np.int8)\n",
    "#\n",
    "# x_val = x_train[8000:9000, :]\n",
    "# x_train = x_train[:8000, :]\n",
    "# x_test = x_test[:1000, :]\n",
    "#\n",
    "# y_val = y_train_[8000:9000]\n",
    "# y_train = y_train_[:8000]\n",
    "# y_test = y_test_[:1000]\n",
    "#\n",
    "# pca = PCA(n_components=10)\n",
    "# x_train = pca.fit_transform(x_train)\n",
    "#\n",
    "# x_val = pca.transform(x_val)\n",
    "# x_test = pca.transform(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# k_range = range(1, 30)\n",
    "# validation_accuracies_euclidean = []\n",
    "# validation_accuracies_manhattan = []\n",
    "# test_accuracies_euclidean = []\n",
    "# test_accuracies_manhattan = []\n",
    "#\n",
    "# # 각각의 거리 척도(euclidean, manhattan)에 대하여 정확도를 계산\n",
    "# for k in k_range:\n",
    "#     knn_euclidean = KNeighborsClassifier(n_neighbors=k, metric='euclidean')\n",
    "#     knn_manhattan = KNeighborsClassifier(n_neighbors=k, metric='manhattan')\n",
    "#     knn_euclidean.fit(x_train, y_train)\n",
    "#     knn_manhattan.fit(x_train, y_train)\n",
    "#     y_pred_euclidean = knn_euclidean.predict(x_val)\n",
    "#     y_pred_manhattan = knn_manhattan.predict(x_val)\n",
    "#     validation_accuracies_euclidean.append(metrics.accuracy_score(y_val, y_pred_euclidean))\n",
    "#     validation_accuracies_manhattan.append(metrics.accuracy_score(y_val, y_pred_manhattan))\n",
    "#\n",
    "#     y_test_pred_euclidean = knn_euclidean.predict(x_test)\n",
    "#     test_accuracies_euclidean.append(metrics.accuracy_score(y_test, y_test_pred_euclidean))\n",
    "#\n",
    "# # 각각의 거리 척도(euclidean, manhattan)에 대하여 비교하는 플롯 출력\n",
    "# fig1 = plt.figure(figsize=(7, 6))\n",
    "# plt.plot(k_range, validation_accuracies_euclidean, label=\"validation_accuracies_euclidean\")\n",
    "# plt.plot(k_range, validation_accuracies_manhattan, label=\"validation_accuracies_manhattan\")\n",
    "#\n",
    "# plt.title('1_1 KNN algorithm_both_metric_validation_accuracies')\n",
    "# plt.xlabel('value of k for KNN')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig1.savefig('fig1.png', dpi=fig1.dpi)\n",
    "#\n",
    "# # 위의 그래프를 확인한 결과 k가 4~6 이후로 accuracy가 감소하며 euclidean 거리 척도에서 더 성능이 좋은 것을 확인하였다.\n",
    "# # k=5, metric='euclidean'인 모델으로 최종 성능을 결정한다.\n",
    "#\n",
    "# knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "# knn.fit(x_train, y_train)\n",
    "# y_pred = knn.predict(x_test)\n",
    "#\n",
    "# fig2 = plt.figure(figsize=(7, 6))\n",
    "# plt.plot(k_range, validation_accuracies_euclidean, label=\"validation_accuracies_euclidean\")\n",
    "# plt.plot(k_range, test_accuracies_euclidean, label=\"test_accuracies_euclidean\")\n",
    "#\n",
    "# plt.title('1_1 KNN algorithm_euclidean_metric_accuracies')\n",
    "# plt.xlabel('value of k for KNN')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig2.savefig('fig2.png', dpi=fig2.dpi)\n",
    "#\n",
    "# print(metrics.accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "#\n",
    "# depth_range = range(1, 40)\n",
    "# validation_accuracies_gini = []\n",
    "# validation_accuracies_entropy = []\n",
    "# test_accuracies_entropy = []\n",
    "#\n",
    "# # gini, entropy에 대하여 정확도를 계산\n",
    "# for d in depth_range:\n",
    "#     tree_gini = DecisionTreeClassifier(criterion='gini', max_depth=d, random_state=100)\n",
    "#     tree_entropy = DecisionTreeClassifier(criterion='entropy', max_depth=d, random_state=100)\n",
    "#\n",
    "#     tree_gini.fit(x_train, y_train)\n",
    "#     tree_entropy.fit(x_train, y_train)\n",
    "#\n",
    "#     y_pred_gini = tree_gini.predict(x_val)\n",
    "#     y_pred_entropy = tree_entropy.predict(x_val)\n",
    "#     y_pred_entropy_test = tree_entropy.predict(x_test)\n",
    "#\n",
    "#     validation_accuracies_gini.append(metrics.accuracy_score(y_val, y_pred_gini))\n",
    "#     validation_accuracies_entropy.append(metrics.accuracy_score(y_val, y_pred_entropy))\n",
    "#     test_accuracies_entropy.append(metrics.accuracy_score(y_test, y_pred_entropy_test))\n",
    "#\n",
    "#\n",
    "# # 각각의 기준(gini, entropy)에 대하여 비교하는 플롯 출력\n",
    "# fig3 = plt.figure(figsize=(7, 6))\n",
    "# plt.title('1_2 decision tree both criterion')\n",
    "# plt.plot(depth_range, validation_accuracies_gini, label=\"gini\")\n",
    "# plt.plot(depth_range, validation_accuracies_entropy, label=\"entropy\")\n",
    "# plt.xlabel('depth')\n",
    "# plt.ylabel('validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig3.savefig('fig3.png', dpi=fig3.dpi)\n",
    "#\n",
    "# # 위의 플롯을 통해 gini 보다 entropy가 정확도가 높음을 알 수 있으며\n",
    "# # depth가 15 이상일때부터 높고 일정한 정확도를 얻음을 알 수 있다.\n",
    "# # 그러므로 depth=15, criterion='entropy'인 모델로 최종 성능을 결정\n",
    "#\n",
    "# tree = DecisionTreeClassifier(criterion='entropy', max_depth=15, random_state=100)\n",
    "# tree.fit(x_train, y_train)\n",
    "#\n",
    "# # 최종 성능 출력\n",
    "# y_pred = tree.predict(x_test)\n",
    "# tree_test = metrics.accuracy_score(y_test, y_pred)\n",
    "# print(tree_test)\n",
    "#\n",
    "# # test, validation 정확도 비교 플롯 출력\n",
    "# fig4 = plt.figure(figsize=(7, 6))\n",
    "# plt.title('1_2 decision tree entropy')\n",
    "# plt.plot(depth_range, validation_accuracies_entropy, label=\"validation_accuracies\")\n",
    "# plt.plot(depth_range, test_accuracies_entropy, label=\"test_accuracies\")\n",
    "# plt.xlabel('depth')\n",
    "# plt.ylabel('validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig4.savefig('fig4.png', dpi=fig4.dpi)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "#\n",
    "# validation_accuracies_svm_linear = []\n",
    "# validation_accuracies_svm_rbf = []\n",
    "# test_accuracies_svm_rbf = []\n",
    "# c_range = [0.001, 0.01, 0.1, 1.0, 10.0, 25.0, 50.0, 100.0]\n",
    "#\n",
    "# # linear, rbf 커널에 대하여 정확도를 계산\n",
    "# for c in c_range :\n",
    "#     svm_linear = SVC(kernel='linear', C=c)\n",
    "#     svm_rbf = SVC(kernel='rbf', C=c)\n",
    "#\n",
    "#     svm_linear.fit(x_train, y_train)\n",
    "#     svm_rbf.fit(x_train, y_train)\n",
    "#\n",
    "#     y_pred_linear = svm_linear.predict(x_val)\n",
    "#     y_pred_rbf = svm_rbf.predict(x_val)\n",
    "#     y_pred_rbf_test = svm_rbf.predict(x_test)\n",
    "#\n",
    "#     validation_accuracies_svm_linear.append(metrics.accuracy_score(y_val, y_pred_linear))\n",
    "#     validation_accuracies_svm_rbf.append(metrics.accuracy_score(y_val, y_pred_rbf))\n",
    "#     test_accuracies_svm_rbf.append(metrics.accuracy_score(y_test, y_pred_rbf_test))\n",
    "#\n",
    "# # 두 커널의 파라미터 C의 변화에 따른 정확도 비교하는 플롯을 출력\n",
    "# fig5 = plt.figure(figsize=(7, 6))\n",
    "# plt.plot(c_range, validation_accuracies_svm_linear, label=\"linear\")\n",
    "# plt.plot(c_range, validation_accuracies_svm_rbf, label=\"rbf\")\n",
    "# plt.ylim(bottom= 0.6)\n",
    "# plt.xscale('symlog')\n",
    "# plt.xlabel('c')\n",
    "# plt.ylabel('validation accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig5.savefig('fig5.png', dpi=fig5.dpi)\n",
    "#\n",
    "# # 위의 플롯을 통해 rbf kernel이 대부분의 경우에 더 효율적임을 알 수 있다.\n",
    "# # 최종 모델은 kernel='rbf', C=10인 모델로 결정한다.\n",
    "#\n",
    "# # 최종 성능 출력\n",
    "# svm = SVC(kernel='rbf', C=10)\n",
    "# svm.fit(x_train, y_train)\n",
    "# y_pred = svm_rbf.predict(x_test)\n",
    "# test_accuracies_svm = metrics.accuracy_score(y_test, y_pred)\n",
    "# print(test_accuracies_svm)\n",
    "#\n",
    "# # test, validation 정확도 비교 플롯 출력\n",
    "# fig6 = plt.figure(figsize=(7, 6))\n",
    "# plt.title('1_3 SVM compare accuracies of rbf')\n",
    "# plt.plot(c_range, validation_accuracies_svm_rbf, label=\"validation_accuracies_svm_rbf\")\n",
    "# plt.plot(c_range, test_accuracies_svm_rbf, label=\"test_accuracies_svm_rbf\")\n",
    "# plt.xlabel('c')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig6.savefig('fig6.png', dpi=fig6.dpi)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# import math\n",
    "#\n",
    "# validation_accuracies_forest_p = []\n",
    "# validation_accuracies_forest_half_p = []\n",
    "# validation_accuracies_forest_sqrt_p = []\n",
    "#\n",
    "# n_range = range(1, 150)\n",
    "# p = x_train.shape[1]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 랜덤 포레스트를 max_featues 파라미터를 세가지의 값(p, 2/p, sqrt(p))를 가지고 비교\n",
    "# for n in n_range :\n",
    "#     # p\n",
    "#     forest_p = RandomForestClassifier(n_estimators=n, random_state=100, max_features=p)\n",
    "#     forest_p.fit(x_train, y_train)\n",
    "#     y_pred_p = forest_p.predict(x_val)\n",
    "#     validation_accuracies_forest_p.append(metrics.accuracy_score(y_val, y_pred_p))\n",
    "#     # p/2\n",
    "#     forest_half_p = RandomForestClassifier(n_estimators=n, random_state=100, max_features=round(p/2))\n",
    "#     forest_half_p.fit(x_train, y_train)\n",
    "#     y_pred_half_p = forest_half_p.predict(x_val)\n",
    "#     validation_accuracies_forest_half_p.append(metrics.accuracy_score(y_val, y_pred_half_p))\n",
    "#\n",
    "#     # sqrt(p)\n",
    "#     forest_sqrt_p = RandomForestClassifier(n_estimators=n, random_state=100, max_features=round(math.sqrt(p)))\n",
    "#     forest_sqrt_p.fit(x_train, y_train)\n",
    "#     y_pred_sqrt_P = forest_sqrt_p.predict(x_val)\n",
    "#     validation_accuracies_forest_sqrt_p.append(metrics.accuracy_score(y_val, y_pred_sqrt_P))\n",
    "#\n",
    "#\n",
    "# fig7 = plt.figure(figsize=(7, 6))\n",
    "# plt.plot(n_range, validation_accuracies_forest_p, label=\"m = p\")\n",
    "# plt.plot(n_range, validation_accuracies_forest_half_p, label=\"m = p/2\")\n",
    "# plt.plot(n_range, validation_accuracies_forest_sqrt_p, label=\"m = sqrt(p)\")\n",
    "# plt.xlabel('n_estimators')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig7.savefig('fig7.png', dpi=fig7.dpi)\n",
    "#\n",
    "# # 위의 플롯을 통해 n이 커질수록 max_features가 전체 feature수(10)의 절반일 때 accuracy가 높은 것을 확인할 수 있다.\n",
    "# # 또한 n=130, max_features=feature수의 절반 일 떄 정확도가 가장 높은 것을 알 수 있다.\n",
    "# # 최종 모델은 max_features=전체 feature 수의 절반값, n=130인 모델로 결정한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# index = validation_accuracies_forest_half_p.index(max(validation_accuracies_forest_half_p))\n",
    "# print(index+1)\n",
    "# print(validation_accuracies_forest_half_p[index])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # 결정된 랜덤 포레스트 모델을 k를 1에서 10까지 k-fold cv를 진행\n",
    "# forest = RandomForestClassifier(n_estimators=95, random_state=100, max_features=round(p/2))\n",
    "#\n",
    "# k_range = range(2, 11)\n",
    "# scores_forest = []\n",
    "#\n",
    "# for k in k_range :\n",
    "#     cv_score = cross_val_score(forest, x_train, y_train, cv=k)\n",
    "#     scores_forest.append(np.mean(cv_score))\n",
    "#\n",
    "# fig8 = plt.figure(figsize=(7, 6))\n",
    "# plt.plot(k_range, scores_forest, label=\"m=p/2, n=95\")\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# fig8.savefig('fig8.png', dpi=fig8.dpi)\n",
    "#\n",
    "# # k-fold(1~10)를 통해 얻은 최적의 test accuracy 출력\n",
    "# best_score_index = scores_forest.index(max(scores_forest))\n",
    "# print(scores_forest[best_score_index])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 회귀분석 데이터 전처리\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv(\"./hw4_2.csv\", encoding='CP949')\n",
    "\n",
    "y = data['가격']\n",
    "y = np.array(y)\n",
    "x = data[['년식',  '마력', '토크', '배기량']]\n",
    "\n",
    "# 정규화\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, shuffle = True)\n",
    "\n",
    "# x_train, x_test을 Dataframe으로 변환\n",
    "X_train = pd.DataFrame(x_train)\n",
    "X_test = pd.DataFrame(x_test)\n",
    "\n",
    "# scikit learn 의 r2_score 함수를 기반으로 adjusted r2 score 계산을 위한 함수\n",
    "def adj_r2(y_real, y_pred, p, n) :\n",
    "    r2_score = metrics.r2_score(y_real, y_pred)\n",
    "    adj_r2_score = 1 - ((n - 1) / (n - p - 1) * (1 - r2_score))\n",
    "    return adj_r2_score\n",
    "\n",
    "test_mse = []\n",
    "test_adj_r2 = []\n",
    "selected_features = []\n",
    "\n",
    "linear_forward = LinearRegression().fit(x_train, y_train)\n",
    "y_pred = linear_forward.predict(x_test)\n",
    "n = x_test.shape[0]\n",
    "p = x_test.shape[1]\n",
    "\n",
    "test_mse.insert(0, metrics.mean_squared_error(y_test, y_pred))\n",
    "test_adj_r2.insert(0, adj_r2(y_test, y_pred, p, n))\n",
    "selected_features.insert(0, list(X_train.columns))\n",
    "\n",
    "# backward selection을 위한 함수\n",
    "def backward_selection(x, y) :\n",
    "    # 처음에는 모든 feature를 포함\n",
    "    included = list(x.columns)\n",
    "\n",
    "    # include 속 feature 개수는 최소 1개 까지\n",
    "    while len(included) > 1:\n",
    "        new_adj_r2 = pd.Series(index=included, dtype=float)\n",
    "        for column in included:\n",
    "            new_included = [x for x in included if x != column]\n",
    "            model = LinearRegression().fit(x[new_included], y)\n",
    "            y_pred = model.predict(X_test[new_included])\n",
    "            n = X_test[new_included].shape[0]\n",
    "            p = X_test[new_included].shape[1]\n",
    "            new_adj_r2[column] = adj_r2(y_test, y_pred, p, n)\n",
    "        test_adj_r2.insert(0, new_adj_r2.max())\n",
    "        test_mse.insert(0, metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "        best_feature = new_adj_r2.index[new_adj_r2.argmax()]\n",
    "        included.remove(best_feature)\n",
    "        selected_features.insert(0, included.copy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# backward_selection 이후 최종 선발된 feature를 출력\n",
    "backward_selection(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 그래프 그리기\n",
    "fig9 = plt.figure(figsize=(7, 6))\n",
    "# plt.plot(range(1, len(result)+1), train_adj_r2_score, label=\"Adjusted R^2\", marker='o')\n",
    "plt.plot(range(0, 4), test_mse, label=\"test_mse\")\n",
    "plt.xticks(np.arange(0, 4, step=1))\n",
    "plt.xlabel('number of feature - 1')\n",
    "plt.ylabel('mse')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig9.savefig('fig9.png', dpi=fig9.dpi)\n",
    "\n",
    "fig10 = plt.figure(figsize=(7, 6))\n",
    "# plt.plot(range(1, len(result)+1), train_adj_r2_score, label=\"Adjusted R^2\", marker='o')\n",
    "plt.plot(range(0, 4), test_adj_r2, label=\"test_adj_r2\")\n",
    "plt.xticks(np.arange(0, 4, step=1))\n",
    "plt.xlabel('number of feature - 1')\n",
    "plt.ylabel('adj_r2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig10.savefig('fig10.png', dpi=fig9.dpi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(selected_features)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 그래프를 통해 feature의 개수가 2개 일 때, Adjusted R^2은 높고, MSE는 낮은 이상적인 상태가 나오므로 이것을 기준으로 모델을 확정\n",
    "# 2개일 때의 feature는 각각 첫번째, 두번째 인 '년식'과 '마력'\n",
    "# 두 feautre 만으로 test Adjusted R^2 와 MSE를 출력\n",
    "print('test Adjusted R^2: ', test_adj_r2[1])\n",
    "print('MSE: ', test_mse[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}