{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:20:49.198177Z",
     "start_time": "2020-11-20T07:20:48.935013Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "import pdb\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:20:50.024781Z",
     "start_time": "2020-11-20T07:20:49.813841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Moon Jeong-Hyeon\n",
      "\n",
      "Last updated: 2020-12-02\n",
      "\n",
      "numpy     : 1.19.3\n",
      "pandas    : 1.1.4\n",
      "matplotlib: 3.3.3\n",
      "sklearn   : 0.23.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Moon Jeong-Hyeon\" -u -d -p numpy,pandas,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:26:00.577262Z",
     "start_time": "2020-11-20T07:25:47.379855Z"
    }
   },
   "outputs": [],
   "source": [
    "mnist_sci = fetch_openml('mnist_784')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:26:00.955279Z",
     "start_time": "2020-11-20T07:26:00.584117Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train_, y_test_ = train_test_split(mnist_sci.data, mnist_sci.target, \n",
    "                                                    test_size = 0.1,\n",
    "                                                   shuffle = True)\n",
    "x_train /= 255.0\n",
    "x_test /= 255.0\n",
    "x_train = x_train.reshape(-1,1,28,28)\n",
    "x_test = x_test.reshape(-1,1,28,28)\n",
    "\n",
    "def one_hoy_label(X):\n",
    "    T = np.zeros((X.size, 10))    \n",
    "    for idx, row in enumerate(T):\n",
    "        row[int(X[idx])] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "y_train = one_hoy_label(y_train_)\n",
    "y_test = one_hoy_label(y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:26:02.151756Z",
     "start_time": "2020-11-20T07:26:02.142811Z"
    }
   },
   "outputs": [],
   "source": [
    "class Adam:\n",
    "\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
    "        \n",
    "        for key in params.keys():\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
    "            \n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\n",
    "            \n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:28:01.604586Z",
     "start_time": "2020-11-20T07:28:01.587781Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
    "                 epochs=20, mini_batch_size=100,\n",
    "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
    "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
    "        self.network = network\n",
    "        self.verbose = verbose\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        self.x_test = x_test\n",
    "        self.t_test = t_test\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = mini_batch_size\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
    "\n",
    "        # optimzer\n",
    "        optimizer_class_dict = {'adam':Adam}\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
    "        \n",
    "        self.train_size = x_train.shape[0]\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
    "        self.current_iter = 0\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        self.train_loss_list = []\n",
    "        self.train_acc_list = []\n",
    "        self.test_acc_list = []\n",
    "\n",
    "    def train_step(self):\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size)\n",
    "        x_batch = self.x_train[batch_mask]\n",
    "        t_batch = self.t_train[batch_mask]\n",
    "        \n",
    "        grads = self.network.gradient(x_batch, t_batch)\n",
    "        self.optimizer.update(self.network.params, grads)\n",
    "        \n",
    "        loss = self.network.loss(x_batch, t_batch)\n",
    "        self.train_loss_list.append(loss)\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\n",
    "        \n",
    "        if self.current_iter % self.iter_per_epoch == 0:\n",
    "            self.current_epoch += 1\n",
    "            \n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\n",
    "                t = self.evaluate_sample_num_per_epoch\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
    "                \n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
    "            self.train_acc_list.append(train_acc)\n",
    "            self.test_acc_list.append(test_acc)\n",
    "\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
    "        self.current_iter += 1\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iter):\n",
    "            self.train_step()\n",
    "\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"=============== Final Test Accuracy ===============\")\n",
    "            print(\"test acc:\" + str(test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:28:55.623183Z",
     "start_time": "2020-11-20T07:28:55.606366Z"
    }
   },
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "#     pdb.set_trace()\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n",
    "\n",
    "\n",
    "def identity_function(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def step_function(x):\n",
    "    return np.array(x > 0, dtype=np.int)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))    \n",
    "\n",
    "\n",
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
    "    \n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def relu_grad(x):\n",
    "    grad = np.zeros(x)\n",
    "    grad[x>=0] = 1\n",
    "    return grad\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "def softmax_loss(X, t):\n",
    "    y = softmax(X)\n",
    "    return cross_entropy_error(y, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:26:04.804811Z",
     "start_time": "2020-11-20T07:26:04.791800Z"
    }
   },
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx\n",
    "\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv & Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:26:05.452385Z",
     "start_time": "2020-11-20T07:26:05.393592Z"
    }
   },
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, prob, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.prob = prob\n",
    "\n",
    "        # 중간 데이터（backward 시 사용）\n",
    "        self.x = None   \n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if np.random.normal() > self.prob:\n",
    "            return self.__forward_col(x)\n",
    "        else: \n",
    "            col_ = self.__forward_col(x)\n",
    "            for_ = self.__forward_for(x)\n",
    "\n",
    "            if np.sum(col_ - for_) > 0.1:                               \n",
    "                raise Exception('error')  \n",
    "\n",
    "            FN, C, FH, FW = self.W.shape\n",
    "            nx = 8\n",
    "            ny = int(np.ceil(FN / nx))\n",
    "\n",
    "            fig = plt.figure()\n",
    "            fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "            for i in range(3):\n",
    "                ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "                ax.imshow(self.W[i, 0], cmap=plt.cm.gray_r)\n",
    "                print(self.W[i, 0,0,1])\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "            return self.__forward_for(x)\n",
    "\n",
    "    def __forward_col(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # 2-2 Convolution(forward) 구현\n",
    "    def __forward_for(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # out size 계산\n",
    "        out_h = 1 + int((H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2 * self.pad - FW) / self.stride)\n",
    "\n",
    "        # padding\n",
    "        x_pad = np.pad(x, ((0, 0), (0, 0), (self.pad, self.pad), (self.pad, self.pad)), 'constant', constant_values=0)\n",
    "\n",
    "        # out 초기화\n",
    "        out = np.zeros((N, FN, out_h, out_w))\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(FN):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        # input의 모든 index에 Filter W를 컨볼루션 진행\n",
    "                        result = x_pad[n, :, (h * self.stride):(h * self.stride + FH), (w * self.stride):(w * self.stride + FW)] * self.W[f]\n",
    "\n",
    "                        # bias 추가 및 out에 저장\n",
    "                        out[n][f][h][w] += np.sum(result) + self.b[f]\n",
    "        return out\n",
    "        \n",
    "    def backward(self, x):\n",
    "        if np.random.normal() > self.prob:\n",
    "            return self.__backward_col(x)\n",
    "        else: \n",
    "            col_ = self.__backward_col(x)\n",
    "            for_ = self.__backward_for(x)\n",
    "\n",
    "            if np.sum(col_ - for_) > 0.1:                               \n",
    "                raise Exception('error')  \n",
    "\n",
    "                \n",
    "            return self.__backward_col(x)    \n",
    "\n",
    "\n",
    "    def __backward_col(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n",
    "    \n",
    "    def __backward_for(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = self.x.shape\n",
    "        out_h = 1 + int((H + 2 * self.pad - FH) / self.stride)\n",
    "        out_w = 1 + int((W + 2 * self.pad - FW) / self.stride)\n",
    "\n",
    "        # padding\n",
    "        x_pad = np.pad(self.x, ((0, 0), (0, 0), (self.pad, self.pad), (self.pad, self.pad)), \"constant\", constant_values=0)\n",
    "\n",
    "        # gradient 변수들 초기화\n",
    "        dx = np.zeros(x_pad.shape)\n",
    "        dw = np.zeros(self.W.shape)\n",
    "        db = np.zeros(self.b.shape)\n",
    "\n",
    "        for n in range(N):\n",
    "            for f in range(FN):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        # Filter에 대한 gradient 계산 및 저장\n",
    "                        dw[f] += x_pad[n, : , h * self.stride : h * self.stride + FH, w * self.stride: w * self.stride + FW] * dout[n][f][h][w]\n",
    "                        # input에 대한 gradient 계산 및 저장\n",
    "                        dx[n, :, h * self.stride: h * self.stride + FH, w * self.stride: w * self.stride + FW] += self.W[f] * dout[n, f, h, w]\n",
    "                # bias에 대한 gradient 계산 및 저장\n",
    "                db[f] = np.sum(dout[n,f,:,:])\n",
    "\n",
    "        # remove padding\n",
    "        dx = dx[:, :, self.pad:dx.shape[2]-self.pad, self.pad:dx.shape[3]-self.pad]\n",
    "\n",
    "        # 이후 gradient 사용을 위해 class의 변수에 저장\n",
    "        self.db = db\n",
    "        self.dW = dw\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0, prob = -12.5):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.prob = prob\n",
    "\n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        if np.random.normal() > self.prob:\n",
    "            return self.__forward_col(x)\n",
    "        else: \n",
    "            col_ = self.__forward_col(x)\n",
    "            for_ = self.__forward_for(x)\n",
    "\n",
    "            if np.sum(col_ - for_) > 0.1:                               \n",
    "                raise Exception('error')      \n",
    "    \n",
    "            return self.__forward_for(x)\n",
    "    \n",
    "    def backward(self, x):\n",
    "        if np.random.normal() > self.prob:\n",
    "            return self.__backward_col(x)\n",
    "        else: \n",
    "            col_ = self.__backward_col(x)\n",
    "            for_ = self.__backward_for(x)\n",
    "\n",
    "            if np.sum(col_ - for_) > 0.1:                               \n",
    "                raise Exception('error')  \n",
    "\n",
    "                \n",
    "            return self.__backward_col(x)    \n",
    "    \n",
    "\n",
    "    def __forward_col(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "\n",
    "        return out\n",
    "    \n",
    "    # 2-3 Pooling(forward) 구현\n",
    "    def __forward_for(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "\n",
    "        # 출력 사이즈 계산\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        # 출력 변수 초기화\n",
    "        out = np.zeros((N, C, out_h, out_w))\n",
    "\n",
    "        for h in range(out_h):\n",
    "            for w in range(out_w):\n",
    "                # maxpooling 진행\n",
    "                out[:, :, h, w] = np.max(self.x[:, :, (h * self.stride):(h * self.stride + self.pool_h), (w * self.stride):(w * self.stride + self.pool_w)])\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __backward_col(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n",
    "    \n",
    "    # 2-3 Pooling(backward) 구현\n",
    "    def __backward_for(self, dout):\n",
    "        N, C, H, W = self.x.shape\n",
    "\n",
    "        # dout의 사이즈 계산\n",
    "        out_h = (H-self.pool_h) // self.stride + 1\n",
    "        out_w = (W-self.pool_w) // self.stride + 1\n",
    "        print(dout.shape, out_h, out_w)\n",
    "\n",
    "        # dx 초기화\n",
    "        dx = np.zeros(self.x.shape)\n",
    "\n",
    "        for n in range(N):\n",
    "            for c in range(C):\n",
    "                for h in range(out_h):\n",
    "                    for w in range(out_w):\n",
    "                        # gradient 계산\n",
    "                        arr = self.x[n, c, (w * self.stride):(w * self.stride + self.pool_h), (h * self.stride):(h * self.stride + self.pool_w)]\n",
    "                        idx = np.nanargmax(arr)\n",
    "                        (arr_w, arr_h) = np.unravel_index(idx, arr.shape)\n",
    "\n",
    "                        # gradient 저장\n",
    "                        dx[n][c][w * self.stride + arr_w][h * self.stride + arr_h] = dout[n][c][w][h]\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:26:06.002453Z",
     "start_time": "2020-11-20T07:26:05.983212Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), prob = -12.5,\n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], prob,\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2, prob = prob)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        # 2-1 Gradient 구현\n",
    "\n",
    "        # backward 진행을 위해 layer의 순서를 반전\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "\n",
    "        # 각 layer의 backward 진행을 통해 dout 갱신\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 각 layer의 weight, bias의 gradient 값을 저장 후 반환\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T07:31:07.074986Z",
     "start_time": "2020-11-20T07:28:59.577043Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.302179953117573\n",
      "=== epoch:1, train acc:0.234375, test acc:0.25 ===\n",
      "train loss:2.3017151087299625\n",
      "train loss:2.3011094295525427\n",
      "train loss:2.3001642597057343\n",
      "train loss:2.2991552669659\n",
      "0.009670146758968944\n",
      "0.0169359698759208\n",
      "0.0061210893343972275\n",
      "train loss:2.297442221619387\n",
      "train loss:2.295836917390992\n",
      "train loss:2.2930782229143856\n",
      "train loss:2.2914659774532393\n",
      "train loss:2.2880161410000066\n",
      "train loss:2.2833997839608955\n",
      "train loss:2.2789633498732687\n",
      "train loss:2.273302398397603\n",
      "train loss:2.2692440317711937\n",
      "train loss:2.264175320336337\n",
      "train loss:2.3127335633447013\n",
      "train loss:2.2495026746225433\n",
      "train loss:2.237864552727742\n",
      "train loss:2.2328182709196343\n",
      "train loss:2.2223260585918196\n",
      "train loss:2.2109138408619784\n",
      "train loss:2.2010967696525063\n",
      "train loss:2.188521737119941\n",
      "train loss:2.170357854264582\n",
      "train loss:2.1582141054866018\n",
      "train loss:2.1406285202290003\n",
      "train loss:2.1121246885619547\n",
      "train loss:2.092947609166544\n",
      "train loss:2.07506894144861\n",
      "train loss:2.044888528663276\n",
      "train loss:2.030411829844675\n",
      "train loss:2.00306322024951\n",
      "train loss:1.9713358368274316\n",
      "train loss:1.923503510749479\n",
      "train loss:1.9061813954932283\n",
      "train loss:1.8627964081605528\n",
      "train loss:1.8131432124024895\n",
      "train loss:1.7825141699234435\n",
      "0.029479214242325552\n",
      "0.036804396323370674\n",
      "0.02604328454649236\n",
      "train loss:1.7485949065450603\n",
      "train loss:1.6965628870871727\n",
      "train loss:1.6587574509642424\n",
      "train loss:1.6290052264322108\n",
      "train loss:1.584342206750886\n",
      "train loss:1.5179456831926073\n",
      "train loss:1.469414622150239\n",
      "train loss:1.429922548924338\n",
      "train loss:1.4099406794893494\n",
      "train loss:1.387205749783769\n",
      "train loss:1.4092911544613207\n",
      "train loss:1.3946153905495657\n",
      "train loss:1.3985866880099853\n",
      "train loss:1.341866303446133\n",
      "0.03917034172729995\n",
      "0.04637893968296248\n",
      "0.03575170759873033\n",
      "train loss:1.3502683507983906\n",
      "train loss:1.3054919534687395\n",
      "train loss:1.3224217838091956\n",
      "train loss:1.264287636862955\n",
      "train loss:1.2737635884186407\n",
      "train loss:1.2236718380113658\n",
      "train loss:1.1755291566255857\n",
      "train loss:1.2102319506185695\n",
      "train loss:1.179169740996961\n",
      "train loss:1.125596655555337\n",
      "train loss:1.078626554840649\n",
      "train loss:1.018064044751724\n",
      "=== epoch:2, train acc:0.5546875, test acc:0.5546875 ===\n",
      "train loss:1.0439153665338075\n",
      "train loss:1.0022929254713475\n",
      "train loss:1.020720615989629\n",
      "0.047463334859779785\n",
      "0.05427859268225314\n",
      "0.04271481969608031\n",
      "train loss:0.9959976036881126\n",
      "train loss:0.9906921126783848\n",
      "train loss:0.9916714307044738\n",
      "train loss:0.9887953604541939\n",
      "train loss:0.9470316736207776\n",
      "train loss:5.455125901497961\n",
      "train loss:0.88329090859435\n",
      "train loss:0.8631526370934626\n",
      "train loss:0.8660480900071641\n",
      "train loss:0.8778958599173429\n",
      "train loss:0.8431066759356599\n",
      "(1000, 10, 12, 12) 12 12\n",
      "train loss:0.8284674450804239\n",
      "train loss:0.8022449664507492\n",
      "train loss:0.8412807409521461\n",
      "train loss:0.8264732686869972\n",
      "train loss:0.7711755514522436\n",
      "train loss:0.7577883865429148\n",
      "train loss:0.8067915955954733\n",
      "train loss:0.7605266904647214\n",
      "train loss:0.7441319226233335\n",
      "train loss:0.7496253498011888\n",
      "train loss:0.7542429991524648\n",
      "train loss:0.7170730783957233\n",
      "train loss:0.7445860553550676\n",
      "train loss:0.6519711575776337\n",
      "train loss:0.7501614589677879\n",
      "train loss:0.6870457626431725\n",
      "train loss:0.6738460302487793\n",
      "train loss:0.6693506205693793\n",
      "train loss:0.6710815492879203\n",
      "train loss:0.6629739649046746\n",
      "train loss:0.6261983397180131\n",
      "(1000, 10, 12, 12) 12 12\n",
      "train loss:0.6261317636660434\n",
      "train loss:0.616344102403251\n",
      "train loss:0.6559568663503736\n",
      "train loss:0.6648486067593206\n",
      "train loss:0.6335123472467411\n",
      "train loss:0.6486123091960635\n",
      "0.0609471298580425\n",
      "0.06760044078887308\n",
      "0.05766410651824266\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAABFCAYAAADuHbzJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADfElEQVR4nO3dO0srURQF4B25xFeCBrQQlMRHI4idjWihFtYWCv4AxU4tRbD1ASLpgmAjotgIWin+AQULGyttbJ1BJJKgZOTcRsMtbjhr4D4X66sGZp3NGVgMIcMkCeecifzv6v72BkR+BRVZKKjIQkFFFgoqslBQkYXCtzjh5uZml8lkvLmXlxd4Znd3N5S7u7uDZ9bX13szlUrFoihKmJm1tbW5XC7nXXN/fw/vIZVKQblkMgnPLJfLUO7p6Sl0zrU3NDS4dDrtzUdRBO8B3S/Sky/Pz89QLgiC0DnX/rNzsYqcyWRscXHRmzs9PYVn7u/vQ7n+/n54Zl9fnzfz8PBQPc7lcnZzc+NdMzk5Ce9hdHQUynV2dsIzb29voVw+n380M0un0zY1NeXNh2EI7wHd7/T0NDzz8PAQyhUKhcda5/TRQiioyEJBRRYKKrJQUJGFgoosFFRkoRDre+QoiiwIAm9uYmICntnb2wvlzs/P4ZkdHR3ezOzsbPW4WCza5eWld02lUoH3UCqVoNzY2Bg8s6mpCcrl83kzM/v4+LDX11dvfnV1Fd7D0tISlIvz3fTR0RGcrUV3ZKGgIgsFFVkoqMhCQUUWCiqyUFCRhYKKLBRUZKGgIguFWI+oy+Uy9ErQ1tYWPLOxsRHKzc/PwzOXl5e9mWKxWD0ulUp2fX3tXTM8PAzv4ezsDMptbGzAM4+Pj+GsmVlPTw/0+Hd8fByeib6WNDMzA89cWVmBcuvr6zXP6Y4sFFRkoaAiCwUVWSioyEJBRRYKKrJQUJGFgoosFGI92UulUjYyMuLNra2twTOHhoagXJwfEERekDw4OKgeh2Foe3t73jULCwvwHubm5qDc29sbPPPq6grOmuFPLAcHB+GZu7u7UA7pyZfW1lY4W4vuyEJBRRYKKrJQUJGFgoosFFRkoaAiCwUVWSioyEJBRRYKsR5RJ5NJ6A8es9ksPHNzcxPKDQwMwDMvLi68mR9fPu3q6rLt7W3vmpaWFngP6J8gvr+/wzPRx/Q7OztmZhYEgRUKBW8+zm8019Vh976TkxN4pn4fWeSTiiwUVGShoCILBRVZKKjIQkFFFgoqslBQkYVCwjmHhxOJwMwef992/qisc67djO66zD6vjfW6fnYiVpFF/lX6aCEUVGShoCILBRVZKKjIQkFFFgoqslBQkYWCiiwUvgPrxdts2n1j6AAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAABFCAYAAADuHbzJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADgElEQVR4nO3du0sjURgF8C/rajQRTGFAUBN7FQRFgiA+QMS/wH9A0cLGysreRjQgWNkrVlrY2KT0VfhoAmkUrNxYGMwIxnC3cFdcVr1nYJ+H86sCc+bjjhyH4GWciHPORP53n/72AkR+BRVZKKjIQkFFFgoqslBQkYXC5zDhWCzmEomEN3d3dwfPTKfTUC6fz8Mzo9GoN1OpVOzp6SliZtbc3Ow6Ojq85xQKBXgNjY2NUK6urg6eGQQBlLu5uSk655K1tbUO+VlUq1V4DTU1NVCuoaEBnoleVxAERedc8q1joYqcSCRsZmbGm9vb24Nnrq+vQ7lMJgPPREp5eXn5Q/7k5MR7zvj4OLyGwcFBKNfW1gbPPD09hXLZbPbK7PkXuqury5svlUrwGpqamqBcd3c3PPPs7AzKHR0dXb13TF8thIKKLBRUZKGgIgsFFVkoqMhCQUUWCqH+jlytVu329tabGxoagmf29vZCue3tbXhmS0uLNzM1NfXyuVQq2f7+vvecSqUCr6FcLkO5kZEReGYsFoNy2Wz2Jd/X1+fNo5scZmY7OztQ7uHhAZ6J/h35I7ojCwUVWSioyEJBRRYKKrJQUJGFgoosFFRkoaAiCwUVWSiE2qIOgsAuLi68ucXFRXhmfX09lJufn4dnzs3NeTP39/cvn8vlsh0eHnrPGRgYgNewu7sL5ZaWluCZW1tbcNbMLJVK2dramjc3NjYGz5ycnIRym5ub8Mzh4WEol8vl3j2mO7JQUJGFgoosFFRkoaAiCwUVWSioyEJBRRYKKrJQCLWzF4/Hrb+/35sLs1vV09MD5UZHR+GZCwsL3szrh1mLxaJtbGx4z5mdnYXXMD09DeXCPKR5cHAAZ83MHh8f7fr62puLx+PwzOPjYyiXSqXgmeju7kd0RxYKKrJQUJGFgoosFFRkoaAiCwUVWSioyEJBRRYKKrJQCLVFHY1GoXfYtba2wjNXV1ehXGdnJzwTeWfe65cUtre32/Lysvcc9B1zZgb9H2mz521k1MTEBJRbWVkxM/yh2jAvrUwm33xf40+Qh5S/Oz8/h7Pv0R1ZKKjIQkFFFgoqslBQkYWCiiwUVGShoCILBRVZKEScc3g4EvliZle/bzl/VNo5lzSjuy6zb9fGel1vHQhVZJF/lb5aCAUVWSioyEJBRRYKKrJQUJGFgoosFFRkoaAiC4WvlF3X8fj88OUAAAAASUVORK5CYII=\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAABFCAYAAADuHbzJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADf0lEQVR4nO3dzyt8exzH8fdcMuRHs6AIGQs2NmyIFHbyZ0ixUJaWdrKwMEsLWynZKBbyB0gWSAkL2bqjkDPjx+hzFy59F9/v9/Oauvd+v/fd87Gamtd59znj5TTN6ZyTCiEY8H/3x69eAPBPoMhwgSLDBYoMFygyXKDIcKGynHBtbW3IZDLR3MPDgzyzo6NDyp2fn8sz0+l0NPP29malUillZtbY2Biy2Wx0m6urK3kNdXV1Uq6qqkqeWSgUpNzt7W0+hNBUWVkZlPnv7+/yGioqKqRcdXW1PPP5+VnKFYvFfAih6XvvlVXkTCZjMzMz0dzOzo48c3V1Vcr19/fLMzs7O6OZ6+vrr9fZbNaOjo6i24yPj8trGB4elnJtbW3yzOPjYymXy+VuzD7+Sbq6uqL5p6cneQ319fVSrru7W555cXEh5U5PT29+9B5fLeACRYYLFBkuUGS4QJHhAkWGCxQZLpT1O3KpVLJ8Ph/NjYyMyDN7e3ul3NbWljyzubk5mpmcnPx6/fj4aHt7e9FtXl9f5TUkSSLlRkdH5Zk1NTVSLpfLfeV7enqiefUkh5nZ/v6+lFNPcpjpvyP/DEdkuECR4QJFhgsUGS5QZLhAkeECRYYLFBkuUGS4QJHhQlmnqAuFgp2cnERzCwsL8kz12q65uTl55uzsbDTz7eU9SZLY4eFhdJuhoSF5Ddvb21JuaWlJnrmxsSFnzT4u+VpfX4/mBgcH5ZkTExNSbnd3V56pXsb2s78RR2S4QJHhAkWGCxQZLlBkuECR4QJFhgsUGS5QZLhQ7t04bWBgIJpbXFyUZ/b19Um5sbExeeb8/Hw0s7m5+fU6n8/b2tpadJvp6Wl5DVNTU1KuWCzKMw8ODuSs2cddNu/v76M59aJWM7OzszMp19LSIs9U7p4awxEZLlBkuECR4QJFhgsUGS5QZLhAkeECRYYLFBkuUGS4UNYp6nQ6bcqDFVtbW+WZKysrUk65z+8n5Zl53z58sb293ZaXl6PbNDQ0yGu4u7uTci8vL/JM9Tl/n59pkiTSae1yHlqpnk6+vLyUZ5bzIM4f4YgMFygyXKDIcIEiwwWKDBcoMlygyHCBIsMFigwXUiEEPZxK/WlmN//ecv5THSGEJjN3+2X297553a/vvVFWkYHfFV8t4AJFhgsUGS5QZLhAkeECRYYLFBkuUGS4QJHhwl+NxdgwHBYmIwAAAABJRU5ErkJggg==\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAABFCAYAAADuHbzJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADhElEQVR4nO3dzSttfRjG8Xt7f0nsQomy2yklJpQZA2XiDzBlYCZjc0mZyMRQBoYmSjI2ESWSEpmQgXQO2bTYtpd1Bs7RU8/Zz+9a9dRznrvvZ7RqXev2W7paaf/aSyqOYwP+70r+6wUA/waKDBcoMlygyHCBIsMFigwXypKEa2tr43Q6Hczd39/LMzOZjJQ7OTmRZ1ZVVQUzhULB3t7eUmZmjY2NsbKO8/NzeQ11dXVSrry8XJ75/Pws5W5ubr7HcdxUVlYWV1RUBPPv7+/yGkpKtGef8nN/KRQKUi6fz3+P47jpd+cSFTmdTtvk5GQwt7GxIc9cXl6Wcn19ffLMzs7OYObs7OzrOJPJ2P7+fvCakZEReQ0DAwNSrrW1VZ55dHQk5RYWFi7NPsvU0dERzD88PMhrqK6ulnLqA8rM7OLiQsqdnp5eFjvHnxZwgSLDBYoMFygyXKDIcIEiwwWKDBcSfY78+vpq19fXwdzg4KA8s6urS8qtr6/LM1taWoKZsbGxr+NcLmdbW1vBa/L5vLyGXC4n5UZHR+WZSTYZzD43hpTP1NUNCTOznZ0dKZfkd6V+jvxPeCLDBYoMFygyXKDIcIEiwwWKDBcoMlygyHCBIsMFigwXEm1RR1FkBwcHwdzMzIw8U912Vb5i9cvU1FQw8/j4+HUcRZHt7e0Fr+nv75fXsLm5KeXm5+flmaurq3LWzCybzdra2low19vbK89Uv8K1vb0tz+zp6ZFyx8fHRc/xRIYLFBkuUGS4QJHhAkWGCxQZLlBkuECR4QJFhgtJ38Yp7W7Nzs7KM9WXEw4NDckzp6eng5m/7njd3t7ayspK8JqJiQl5DePj41JOfcOmmdnu7q6cNTP7+PiwKIqCuZqaGnmm+kbS5uZmeWaSN5IWwxMZLlBkuECR4QJFhgsUGS5QZLhAkeECRYYLFBkuUGS4kGiLurKy0rLZbDDX3t4uz1xcXJRy3d3d8kzlC7JPT09fx21tbTY3Nxe8pr6+Xl7D3d2dlHt5eZFnDg8PS7mlpSUz+7zHw8PDYL60tFReQ0NDg5S7vCz6L/H+5urqSs4WwxMZLlBkuECR4QJFhgsUGS5QZLhAkeECRYYLFBkupOI41sOp1Dcz07ds/mztcRw3mbm7L7Of9+b1vn53IlGRgT8Vf1rABYoMFygyXKDIcIEiwwWKDBcoMlygyHCBIsOFH9qr2c5zRmTrAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 3 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAALIAAABFCAYAAADuHbzJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAADe0lEQVR4nO3dTyh0fRjG8ZsZTM2UmtiITIooC0VsreyUtZKtpYWsFUtZy0ZZWdmQnT0SycKCNHam5/E3g5kpv3fxvGb1PO/vOm/v37vvZ3XqXOfud3I56RxzpiGEYMD/XeO/vQDgr0CR4QJFhgsUGS5QZLhAkeFCOkk4l8uFfD4fzT09Pckzu7u7pdzl5aU8M5PJRDOVSsVqtVqDmVlbW1soFArRY66vr+U1ZLNZKdfU1CTP/Pj4kHKlUul7CKE9lUqFdDr+I/78/JTX0NioXfuSnFetVpNy1Wr1ewih/Wf7EhU5n8/b4uJiNLezsyPP3NjYkHIjIyPyzIGBgWjm4uKivl0oFOzk5CR6zOTkpLyGsbExKdfR0SHPVH+ZV1dXb83M0um0NP/9/V1eQ0tLi5RLcl53d3dSrlgs3v5qH39awAWKDBcoMlygyHCBIsMFigwXKDJcSHQfuVKp2M3NTTSn3kM1M+vt7ZVyu7u78syurq5oZnp6ur79/Pxse3t70WPK5bK8hsfHRyk3NTUlz2xubpazZj/u+fb09ERz1WpVnnl+fi7lksxU7yP/Ea7IcIEiwwWKDBcoMlygyHCBIsMFigwXKDJcoMhwgSLDhUSPqMvlsh0fH0dzS0tL8sxUKiXl5ubm5Jnz8/PRzMvLS3379fXVDg8Po8cMDQ3Ja9jf35dya2tr8szNzU05a2bW19dnBwcH0Zzy0bAvw8PDUu709FSeqf6bwtXV1S/3cUWGCxQZLlBkuECR4QJFhgsUGS5QZLhAkeECRYYLiZ7sZbNZGx0djeaWl5flmeqTovHxcXnmwsJCNLO9vV3fvr+/t62tregxs7Oz8hpmZmaknPqGTTOzo6MjOWtmFkKQ3nSpvL30S7FYlHLKW1u/KG8MjeGKDBcoMlygyHCBIsMFigwXKDJcoMhwgSLDBYoMFygyXEj0bDCTyVh/f380p7yT94v64cvBwUF55tnZWTTz9vZW3+7s7LSVlZXoMblcTl7Dw8ODlFO/LNHMbGJiQsqtr6+b2Y9zVL4/MInW1lYpl+Sdx6VS6c8up44rMlygyHCBIsMFigwXKDJcoMhwgSLDBYoMFygyXGgIIejhhoZvZnb79y3nH9UdQmg3c3deZr+fm9fz+tmOREUG/qv40wIuUGS4QJHhAkWGCxQZLlBkuECR4QJFhgsUGS78BiHm2bPRQzhSAAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "network = SimpleConvNet(input_dim=(1,28,28), prob = -2, \n",
    "                        conv_param = {'filter_num': 10, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, y_train, x_test, y_test,\n",
    "                  epochs=max_epochs, mini_batch_size=1000,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.0005},\n",
    "                  evaluate_sample_num_per_epoch=128)\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}